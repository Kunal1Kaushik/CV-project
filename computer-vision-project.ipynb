{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711b7542",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:17.386254Z",
     "iopub.status.busy": "2024-05-13T13:01:17.385318Z",
     "iopub.status.idle": "2024-05-13T13:01:18.465138Z",
     "shell.execute_reply": "2024-05-13T13:01:18.463787Z"
    },
    "papermill": {
     "duration": 1.089074,
     "end_time": "2024-05-13T13:01:18.468403",
     "exception": false,
     "start_time": "2024-05-13T13:01:17.379329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b09210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:18.477507Z",
     "iopub.status.busy": "2024-05-13T13:01:18.476981Z",
     "iopub.status.idle": "2024-05-13T13:01:30.669325Z",
     "shell.execute_reply": "2024-05-13T13:01:30.667282Z"
    },
    "papermill": {
     "duration": 12.200178,
     "end_time": "2024-05-13T13:01:30.672288",
     "exception": false,
     "start_time": "2024-05-13T13:01:18.472110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 14375389.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 470519.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 3812351.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2528943.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033368c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:30.687134Z",
     "iopub.status.busy": "2024-05-13T13:01:30.686275Z",
     "iopub.status.idle": "2024-05-13T13:01:44.994701Z",
     "shell.execute_reply": "2024-05-13T13:01:44.993690Z"
    },
    "papermill": {
     "duration": 14.319446,
     "end_time": "2024-05-13T13:01:44.997908",
     "exception": false,
     "start_time": "2024-05-13T13:01:30.678462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = []\n",
    "dataset2 = []\n",
    "dataset3 = []\n",
    "dataset4 = []\n",
    "dataset5 = []\n",
    "dataset6 = []\n",
    "dataset7 = []\n",
    "dataset8 = []\n",
    "dataset9 = []\n",
    "dataset10 = []\n",
    "\n",
    "for i in range(6000):\n",
    "    \n",
    "    dataset1.append(trainset[i])\n",
    "    \n",
    "for i in range(6000, 12000):\n",
    "    \n",
    "    dataset2.append(trainset[i])\n",
    "    \n",
    "for i in range(12000, 18000):\n",
    "    \n",
    "    dataset3.append(trainset[i])\n",
    "    \n",
    "for i in range(18000, 24000):\n",
    "    \n",
    "    dataset4.append(trainset[i])\n",
    "    \n",
    "for i in range(24000, 30000):\n",
    "    \n",
    "    dataset5.append(trainset[i])\n",
    "for i in range(30000, 36000):\n",
    "    \n",
    "    dataset6.append(trainset[i])\n",
    "    \n",
    "for i in range(36000, 42000):\n",
    "    \n",
    "    dataset7.append(trainset[i])\n",
    "    \n",
    "for i in range(42000, 48000):\n",
    "    \n",
    "    dataset8.append(trainset[i])\n",
    "    \n",
    "for i in range(48000, 54000):\n",
    "    \n",
    "    dataset9.append(trainset[i])\n",
    "    \n",
    "for i in range(54000, 60000):\n",
    "    \n",
    "    dataset10.append(trainset[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09641212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:45.016052Z",
     "iopub.status.busy": "2024-05-13T13:01:45.014739Z",
     "iopub.status.idle": "2024-05-13T13:01:45.026838Z",
     "shell.execute_reply": "2024-05-13T13:01:45.025376Z"
    },
    "papermill": {
     "duration": 0.024505,
     "end_time": "2024-05-13T13:01:45.030164",
     "exception": false,
     "start_time": "2024-05-13T13:01:45.005659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# load the training data\n",
    "trainloader1 = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=False)\n",
    "trainloader2 = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False)\n",
    "trainloader3 = torch.utils.data.DataLoader(dataset3, batch_size=batch_size, shuffle=False)\n",
    "trainloader4 = torch.utils.data.DataLoader(dataset4, batch_size=batch_size, shuffle=False)\n",
    "trainloader5 = torch.utils.data.DataLoader(dataset5, batch_size=batch_size, shuffle=False)\n",
    "trainloader6 = torch.utils.data.DataLoader(dataset6, batch_size=batch_size, shuffle=False)\n",
    "trainloader7 = torch.utils.data.DataLoader(dataset7, batch_size=batch_size, shuffle=False)\n",
    "trainloader8 = torch.utils.data.DataLoader(dataset8, batch_size=batch_size, shuffle=False)\n",
    "trainloader9 = torch.utils.data.DataLoader(dataset9, batch_size=batch_size, shuffle=False)\n",
    "trainloader10 = torch.utils.data.DataLoader(dataset10, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38223fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:45.046699Z",
     "iopub.status.busy": "2024-05-13T13:01:45.046150Z",
     "iopub.status.idle": "2024-05-13T13:01:45.069348Z",
     "shell.execute_reply": "2024-05-13T13:01:45.068060Z"
    },
    "papermill": {
     "duration": 0.035015,
     "end_time": "2024-05-13T13:01:45.072218",
     "exception": false,
     "start_time": "2024-05-13T13:01:45.037203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and load the testset\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d06704e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T13:01:45.087119Z",
     "iopub.status.busy": "2024-05-13T13:01:45.086657Z",
     "iopub.status.idle": "2024-05-13T13:01:45.677419Z",
     "shell.execute_reply": "2024-05-13T13:01:45.675820Z"
    },
    "papermill": {
     "duration": 0.601918,
     "end_time": "2024-05-13T13:01:45.680322",
     "exception": true,
     "start_time": "2024-05-13T13:01:45.078404",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# get some random training images\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrainloader\u001b[49m)\n\u001b[1;32m     13\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# show images\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.22408,
   "end_time": "2024-05-13T13:01:48.304872",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-13T13:01:14.080792",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
